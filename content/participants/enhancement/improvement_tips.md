---
date: '2025-10-30T14:00:00+09:00'
draft: false
title: '人狼知能エージェントの課題点'
category: participants_guide
---

このページでは人狼知能エージェント（特にLLMベース）の**現状の課題**を、2023→2024→2025のOverview論文に基づいて整理しました。各年で新たに見えた点も含め、**共通する課題／年度固有の課題**を区別してまとめます。エージェントの改良を行う際の「どこに注目すべきか」の地図として活用してください。

* [INLG 2023 overview](https://aclanthology.org/2023.inlg-genchal.13/)
* [INLG 2024 overview](https://aclanthology.org/2024.aiwolfdial-1.1/)
* INLG 2025 overview ※ACL公式サイトでの更新があり次第、こちらにもリンクを共有します。

---

## 3年間に共通して指摘されている主要課題

* **発話と行動の不一致（Talk–Action Inconsistency）**
  生成された発話内容が投票・占い・襲撃などの**実際の行動と矛盾**する例が継続的に見られます。特に、2023・2024の両年で明記され、2024論文でも「改善はあるが未解決」とされています。

* **一貫性・長文脈・共通基盤（common ground）・論理の扱い**
  **長い文脈を跨いだ整合性**や**論理性**、**共有知識の維持**は依然として難題と総括されています（導入部で毎年言及）。

* **「嘘（欺瞞）」＝二重状態の維持の困難さ**
  エージェントが**外面（ゲーム内宣言）と内面（真の目的）を同時に保ち続ける**ことの難しさは、将来課題として2023・2024の両年で強調。

* **応答速度（レイテンシ）**
  LLMベースの応答が **人間より遅い（ときに数分）** という技術課題が繰り返し指摘されています。

* **評価法の難しさ**
  2023は**勝率評価を見送る**理由（試行数不足、会話理解前提の未達など）を明記。一方で**主観評価／ログ分析**は通年で実施されています。

---

## 年度別に新たに浮き彫りになった／強調された論点

### 2023年（第5回・自然言語部門の再編初期）

* **Talk–Actionの不整合**・**論理的役職推定の不十分さ**を詳細に指摘。**勝率は未採用**（統計的に有意な試行数が確保できない等の理由）。
* **将来課題**として「**嘘の二重性**」「人狼同士の**Whisper導入**」「**5人超え**の多人数戦」などが示唆されました。

### 2024年（第6回・改善は進むが未解決点が残存）

* **要約・ペルソナ・LLM外部のロジック**など**新しい工夫**が登場し、**人手主観評価＋勝率＋ログ分析**で評価（勝率も試行）。それでも**発話と行動の不整合**は残存。
* **応答時間の遅さ**（ときに**数分**）が技術課題として明記。

### 2025年（第7回・**13人戦トラックの新設**で複雑性が一気に増加）

* **13人戦＋人狼のWhisper**（秘密会話）を導入。**チームとしての協調**が求められる環境で、**コミュニケーションの流暢さ・文脈意識が想定より低い**ことが確認され、**複数エージェント間の複雑関係**への弱さが露わに。
* 評価面では、**勝率・人手主観評価に加えてLLM-as-a-Judge**を**新規導入**。

---

## 課題マップ（どの年で強く観測されたか）

| 課題カテゴリ               | 2023     | 2024    | 2025           | 補足                               |
| -------------------- | -------- | ------- | -------------- | -------------------------------- |
| 発話と行動の不一致            | ◎        | ◎       | （継続懸念）         | 23・24で明記。25は文脈側の不足が前面に。          |
| 長文脈・論理・common ground | ○        | ○       | ◎              | 25は**多者関係**で特に顕在化。               |
| 「嘘」の二重性（欺瞞の維持）       | ○        | ○       | （重要度上昇）        | 23・24で将来課題。13人戦・Whisperで研究価値増。   |
| 応答速度（レイテンシ）          | △        | ○       | △              | 24で「**ときに数分**」と明記。               |
| 評価設計（勝率・主観・自動評価）     | △（勝率見送り） | ○（勝率採用） | ◎（LLM Judge追加） | 23→24→25で評価多角化。                  |

> **凡例**：◎=強く指摘／新規性あり、○=明記されている、△=補足的に触れられる

---

## 実装・研究のための「着眼ポイント」一覧

> ここに挙げるのは**課題の整理**です（解決策の押し付けではありません）。各チームで自由に発想してください。

1. **Talk–Action整合性の担保**

   * 発話内容（宣言や推論）と、**投票・占い・襲撃などの最終行動が矛盾しない**こと。評価ログで**矛盾パターン**を洗い出しやすく。

2. **長文脈・共通基盤・論理**

   * **ターンを跨ぐ記憶・一貫性**、他者発言の反映、**共有事実（CO等）**の保持。多数プレイヤー（13人）では**相互関係の追跡**が鍵。

3. **欺瞞（嘘）のモデル化**

   * **外向きの目的**と**内的ゴール**の二重管理。Whisper環境や多人数戦での**チーム内・対外整合**の観点で再検討。

4. **応答時間の短縮**

   * 実運用上の**遅延**はゲーム体験に直結。ログ基盤やプロンプト設計、前処理（要約など）を含む**システム側要因**も観測対象に。

5. **評価設計の多角化**

   * **勝率・人手主観・LLM Judge**を**補完的**に使い分け、**サンプル数やバイアス**の影響を注視。23→24→25の**評価進化**を踏まえ、手元の検証プロトコルを整備。  

---

以上で現在の人狼知能エージェントの抱えている課題点の整理およびエージェント改良の着眼点の整理が完了しました。

---

[参加者マニュアルトップへ](../_index.md)\
[改良・拡張トップへ](./_index.md)\
[前へ（LLM Judgeの使いかた）](../evaluation/llm_judge_usage.md)\
[次へ（サンプルエージェントの構成と主要ファイル）](./nlp_agent_structure.md)
